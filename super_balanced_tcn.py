#!/usr/bin/env python3
"""
üéØ SUPER BALANCED TCN - Implementaci√≥n final con t√©cnicas avanzadas

Basado en el documento compartido, implementa:
‚úÖ Entrenamiento por r√©gimen de mercado separado
‚úÖ Validaci√≥n walk-forward con ventanas deslizantes  
‚úÖ Aumentaci√≥n de datos con noise injection
‚úÖ Funci√≥n de p√©rdida Sharpe Ratio adaptativa
‚úÖ Multi-timeframe analysis (5m, 15m, 1h)
‚úÖ Forzado de balance absoluto con t√©cnicas profesionales
‚úÖ Arquitectura TCN optimizada con dilataci√≥n exponencial
"""

import asyncio
import numpy as np
import pandas as pd
import tensorflow as tf
from binance.client import Client as BinanceClient
from sklearn.preprocessing import MinMaxScaler, RobustScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import TimeSeriesSplit
from imblearn.over_sampling import SMOTE
from collections import Counter
import pickle
import json
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class StableClassificationLoss(tf.keras.losses.Loss):
    """Loss function estable para clasificaci√≥n balanceada con penalizaci√≥n de bias"""
    def __init__(self, alpha=0.8, beta=0.2, name='stable_balanced_loss'):
        super().__init__(name=name)
        self.alpha = alpha  # Peso para classification loss
        self.beta = beta    # Peso para balance penalty
    
    def call(self, y_true, y_pred):
        # === LOSS DE CLASIFICACI√ìN ESTABLE ===
        # Clipping para evitar log(0) 
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)
        
        # Cross entropy loss estable
        ce_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
        ce_loss = tf.reduce_mean(ce_loss)
        
        # === PENALTY POR DESBALANCE ===
        # Calcular distribuci√≥n de predicciones
        pred_probs = tf.reduce_mean(y_pred, axis=0)  # [p_sell, p_hold, p_buy]
        
        # Penalizar desviaci√≥n de balance perfecto (33.33% cada clase)
        target_dist = tf.constant([1.0/3.0, 1.0/3.0, 1.0/3.0])
        balance_penalty = tf.reduce_sum(tf.square(pred_probs - target_dist))
        
        # === PENALTY POR OVERCONFIDENCE ===
        # Penalizar predicciones muy extremas (overconfident)
        max_pred = tf.reduce_max(y_pred, axis=1)
        overconfidence_penalty = tf.reduce_mean(tf.maximum(0.0, max_pred - 0.8))
        
        # === LOSS COMBINADO ESTABLE ===
        total_loss = (self.alpha * ce_loss + 
                     self.beta * balance_penalty + 
                     0.1 * overconfidence_penalty)
        
        # Verificar estabilidad num√©rica
        total_loss = tf.where(tf.math.is_finite(total_loss), total_loss, ce_loss)
        
        return total_loss


class SuperBalancedTCN:
    """
    üöÄ Implementaci√≥n profesional de TCN Anti-Bias con t√©cnicas avanzadas
    """
    
    def __init__(self):
        self.symbol = "BTCUSDT"
        self.lookback_window = 60
        self.expected_features = 66
        self.class_names = ['SELL', 'HOLD', 'BUY']
        
        # Multi-timeframe seg√∫n documento
        self.timeframes = {
            "5m": 600,   # 50 horas de datos
            "15m": 400,  # 100 horas de datos  
            "1h": 168    # 1 semana de datos
        }
        
        print("üéØ SUPER BALANCED TCN - Versi√≥n Profesional")
        print("="*60)
        print("üìã Configuraci√≥n avanzada:")
        print(f"   - Multi-timeframe: {list(self.timeframes.keys())}")
        print(f"   - Features: {self.expected_features}")
        print(f"   - Ventana temporal: {self.lookback_window}")
        print("   - T√©cnicas profesionales aplicadas ‚úÖ")
    
    def setup_binance_client(self) -> bool:
        """Setup optimizado de Binance"""
        try:
            print("\nüîó Conectando a Binance (Testnet/Mainnet)...")
            self.binance_client = BinanceClient()
            
            # Test de conectividad
            server_time = self.binance_client.get_server_time()
            print(f"‚úÖ Conectado - Server time: {datetime.fromtimestamp(server_time['serverTime']/1000)}")
            return True
        except Exception as e:
            print(f"‚ùå Error de conexi√≥n: {e}")
            return False
    
    def collect_professional_data(self) -> Dict[str, pd.DataFrame]:
        """Recolecci√≥n de datos profesional multi-timeframe"""
        try:
            print("\nüìä Recolectando datos multi-timeframe profesionales...")
            
            timeframe_data = {}
            
            for interval, limit in self.timeframes.items():
                print(f"   üìà Descargando {interval} - {limit} per√≠odos...")
                
                try:
                    klines = self.binance_client.get_historical_klines(
                        symbol=self.symbol,
                        interval=interval,
                        limit=limit
                    )
                    
                    if not klines:
                        print(f"     ‚ö†Ô∏è No hay datos para {interval}")
                        continue
                    
                    df = pd.DataFrame(klines, columns=[
                        'timestamp', 'open', 'high', 'low', 'close', 'volume',
                        'close_time', 'quote_asset_volume', 'number_of_trades',
                        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
                    ])
                    
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'number_of_trades']
                    
                    for col in numeric_cols:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    df = df.dropna()
                    
                    if len(df) >= 100:  # M√≠nimo 100 per√≠odos
                        timeframe_data[interval] = df
                        print(f"     ‚úÖ {interval}: {len(df)} per√≠odos v√°lidos")
                    else:
                        print(f"     ‚ùå {interval}: Datos insuficientes ({len(df)})")
                    
                except Exception as e:
                    print(f"     ‚ùå Error en {interval}: {e}")
            
            if not timeframe_data:
                print("‚ùå No se pudo obtener datos de ning√∫n timeframe")
                return {}
            
            print(f"\n‚úÖ Datos recolectados: {len(timeframe_data)} timeframes")
            return timeframe_data
            
        except Exception as e:
            print(f"‚ùå Error general: {e}")
            return {}
    
    def detect_professional_regimes(self, df: pd.DataFrame) -> pd.DataFrame:
        """Detecci√≥n profesional de reg√≠menes seg√∫n documento"""
        try:
            print("\nüîç Detectando reg√≠menes profesionales...")
            
            df = df.copy()
            
            # === AN√ÅLISIS MULTI-PER√çODO ===
            periods = [5, 10, 20, 50]
            
            for period in periods:
                df[f'returns_{period}'] = df['close'].pct_change(periods=period)
                df[f'vol_{period}'] = df[f'returns_{period}'].rolling(period).std()
                df[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
            
            # === INDICADORES T√âCNICOS AVANZADOS ===
            # RSI multi-per√≠odo
            for period in [14, 21]:
                delta = df['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(period).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
                df[f'rsi_{period}'] = 100 - (100 / (1 + gain / loss))
            
            # MACD profesional
            ema12 = df['close'].ewm(span=12).mean()
            ema26 = df['close'].ewm(span=26).mean()
            df['macd'] = ema12 - ema26
            df['macd_signal'] = df['macd'].ewm(span=9).mean()
            df['macd_histogram'] = df['macd'] - df['macd_signal']
            
            # Bollinger Bands
            bb_period = 20
            bb_middle = df['close'].rolling(bb_period).mean()
            bb_std = df['close'].rolling(bb_period).std()
            df['bb_upper'] = bb_middle + (bb_std * 2)
            df['bb_lower'] = bb_middle - (bb_std * 2)
            df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
            
            # Volume analysis
            df['volume_ma_20'] = df['volume'].rolling(20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_ma_20']
            
            # === CLASIFICACI√ìN PROFESIONAL ===
            regimes = []
            
            for i, row in df.iterrows():
                # Usar m√∫ltiples indicadores para robustez
                ret_20 = row['returns_20']
                vol_20 = row['vol_20']
                rsi_14 = row['rsi_14']
                macd_hist = row['macd_histogram']
                bb_pos = row['bb_position']
                vol_ratio = row['volume_ratio']
                momentum_50 = row['momentum_50']
                
                if pd.isna(ret_20) or pd.isna(vol_20):
                    regime = 1  # SIDEWAYS por defecto
                else:
                    # === CONDICIONES BEAR ===
                    bear_signals = 0
                    if ret_20 < -0.015:  # -1.5% return
                        bear_signals += 2
                    if rsi_14 < 35:  # RSI bajista
                        bear_signals += 1
                    if macd_hist < -0.0001:  # MACD negativo
                        bear_signals += 1
                    if bb_pos < 0.2:  # Cerca del BB inferior
                        bear_signals += 1
                    if momentum_50 < -0.02:  # Momentum negativo
                        bear_signals += 1
                    
                    # === CONDICIONES BULL ===
                    bull_signals = 0
                    if ret_20 > 0.015:  # +1.5% return
                        bull_signals += 2
                    if rsi_14 > 65:  # RSI alcista
                        bull_signals += 1
                    if macd_hist > 0.0001:  # MACD positivo
                        bull_signals += 1
                    if bb_pos > 0.8:  # Cerca del BB superior
                        bull_signals += 1
                    if momentum_50 > 0.02:  # Momentum positivo
                        bull_signals += 1
                    if vol_ratio > 1.5:  # Volumen alto (inter√©s)
                        bull_signals += 1
                    
                    # === DECISI√ìN FINAL ===
                    if bear_signals >= 3:
                        regime = 0  # BEAR
                    elif bull_signals >= 3:
                        regime = 2  # BULL
                    else:
                        regime = 1  # SIDEWAYS
                
                regimes.append(regime)
            
            df['regime'] = regimes
            
            # === ESTAD√çSTICAS Y BALANCE FORZADO ===
            regime_counts = Counter(regimes)
            total = len(regimes)
            
            print("üìä Distribuci√≥n inicial de reg√≠menes:")
            regime_names = ['BEAR', 'SIDEWAYS', 'BULL']
            for i, name in enumerate(regime_names):
                count = regime_counts[i]
                pct = count / total * 100
                print(f"   - {name}: {count} ({pct:.1f}%)")
            
            # === BALANCE FORZADO ===
            min_count = min(regime_counts.values())
            max_count = max(regime_counts.values())
            
            if max_count / min_count > 1.8:  # Si hay desbalance > 80%
                print("\n‚öñÔ∏è Aplicando balance forzado de reg√≠menes...")
                
                target_count = int(total * 0.3)  # Cada r√©gimen ~30%
                balanced_dfs = []
                
                for regime_id in [0, 1, 2]:
                    regime_data = df[df['regime'] == regime_id]
                    
                    if len(regime_data) > target_count:
                        # Undersample
                        balanced_data = regime_data.sample(n=target_count, random_state=42)
                    elif len(regime_data) < target_count * 0.7:
                        # Oversample duplicando con ruido
                        n_needed = target_count - len(regime_data)
                        
                        if len(regime_data) > 0:
                            oversampled = []
                            for _ in range(n_needed):
                                sample = regime_data.sample(n=1, random_state=42).copy()
                                # A√±adir ruido m√≠nimo a precios
                                noise = np.random.normal(0, 0.0001, 1)[0]
                                sample['close'] *= (1 + noise)
                                sample['open'] *= (1 + noise)
                                sample['high'] *= (1 + noise)
                                sample['low'] *= (1 + noise)
                                oversampled.append(sample)
                            
                            regime_data = pd.concat([regime_data] + oversampled, ignore_index=True)
                        
                        balanced_data = regime_data
                    else:
                        balanced_data = regime_data
                    
                    balanced_dfs.append(balanced_data)
                
                df = pd.concat(balanced_dfs, ignore_index=True)
                df = df.sample(frac=1, random_state=42).reset_index(drop=True)
                
                # Nuevas estad√≠sticas
                final_counts = Counter(df['regime'])
                final_total = len(df)
                
                print("‚úÖ Balance final aplicado:")
                for i, name in enumerate(regime_names):
                    count = final_counts[i]
                    pct = count / final_total * 100
                    print(f"   - {name}: {count} ({pct:.1f}%)")
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return df
    
    def apply_professional_augmentation(self, df: pd.DataFrame) -> pd.DataFrame:
        """Aumentaci√≥n profesional de datos seg√∫n documento"""
        try:
            print("\nüîß Aplicando aumentaci√≥n profesional...")
            
            original_len = len(df)
            augmented_dfs = [df.copy()]
            
            # === NOISE INJECTION PROFESIONAL ===
            noise_configs = [
                {"price_noise": 0.0005, "volume_noise": 0.01, "weight": 0.3},  # Ruido suave
                {"price_noise": 0.001, "volume_noise": 0.02, "weight": 0.2},   # Ruido medio
                {"price_noise": 0.002, "volume_noise": 0.05, "weight": 0.1},   # Ruido fuerte
            ]
            
            for config in noise_configs:
                n_samples = int(original_len * config["weight"])
                
                if n_samples > 0:
                    # Seleccionar muestra estratificada por r√©gimen
                    samples_per_regime = n_samples // 3
                    regime_samples = []
                    
                    for regime_id in [0, 1, 2]:
                        regime_data = df[df['regime'] == regime_id]
                        if len(regime_data) >= samples_per_regime:
                            regime_sample = regime_data.sample(n=samples_per_regime, random_state=42)
                            regime_samples.append(regime_sample)
                    
                    if regime_samples:
                        sample_df = pd.concat(regime_samples, ignore_index=True)
                        
                        # Aplicar ruido
                        df_noisy = sample_df.copy()
                        
                        # Ruido en precios (correlacionado)
                        price_noise = np.random.normal(0, config["price_noise"], len(df_noisy))
                        df_noisy['open'] *= (1 + price_noise)
                        df_noisy['high'] *= (1 + price_noise * 0.8)  # Menos ruido en high
                        df_noisy['low'] *= (1 + price_noise * 0.8)   # Menos ruido en low
                        df_noisy['close'] *= (1 + price_noise)
                        
                        # Ruido en volumen (independiente)
                        vol_noise = np.random.normal(0, config["volume_noise"], len(df_noisy))
                        df_noisy['volume'] *= (1 + vol_noise)
                        
                        # Redetectar reg√≠menes con datos aumentados
                        df_noisy = self.detect_professional_regimes(df_noisy)
                        
                        augmented_dfs.append(df_noisy)
            
            # === COMBINACI√ìN FINAL ===
            df_augmented = pd.concat(augmented_dfs, ignore_index=True)
            df_augmented = df_augmented.sample(frac=1, random_state=42).reset_index(drop=True)
            
            print(f"‚úÖ Aumentaci√≥n completada:")
            print(f"   - Muestras originales: {original_len}")
            print(f"   - Muestras finales: {len(df_augmented)}")
            print(f"   - Factor de aumento: {len(df_augmented)/original_len:.1f}x")
            
            return df_augmented
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return df
    
    def create_super_balanced_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """Crea labels s√∫per balanceadas con t√©cnicas profesionales"""
        try:
            print("\nüéØ Creando labels s√∫per balanceadas...")
            
            df = df.copy()
            
            # === HORIZONTES M√öLTIPLES ===
            horizons = [6, 12, 24]  # 30min, 1h, 2h
            
            for h in horizons:
                df[f'future_price_{h}'] = df['close'].shift(-h)
                df[f'change_{h}'] = (df[f'future_price_{h}'] - df['close']) / df['close']
            
            # === UMBRALES ADAPTATIVOS POR R√âGIMEN ===
            regime_thresholds = {
                0: {"sell": -0.001, "buy": 0.002},  # BEAR: sell f√°cil, buy dif√≠cil
                1: {"sell": -0.002, "buy": 0.002},  # SIDEWAYS: sim√©trico
                2: {"sell": -0.002, "buy": 0.001},  # BULL: buy f√°cil, sell dif√≠cil
            }
            
            labels = []
            label_justifications = []
            
            for i, row in df.iterrows():
                regime = int(row['regime'])
                changes = [row[f'change_{h}'] for h in horizons]
                
                if any(pd.isna(changes)):
                    labels.append(1)  # HOLD
                    label_justifications.append("MISSING_DATA")
                    continue
                
                thresholds = regime_thresholds[regime]
                
                # === SISTEMA DE VOTACI√ìN PONDERADA ===
                sell_votes = 0
                buy_votes = 0
                weights = [0.5, 0.3, 0.2]  # M√°s peso a horizontes cortos
                
                for j, change in enumerate(changes):
                    if change < thresholds["sell"]:
                        sell_votes += weights[j]
                    elif change > thresholds["buy"]:
                        buy_votes += weights[j]
                
                # === DECISI√ìN FINAL ===
                if sell_votes >= 0.6:  # 60% de confianza
                    label = 0  # SELL
                    justification = f"SELL_CONF_{sell_votes:.1f}"
                elif buy_votes >= 0.6:  # 60% de confianza
                    label = 2  # BUY
                    justification = f"BUY_CONF_{buy_votes:.1f}"
                else:
                    label = 1  # HOLD
                    justification = f"HOLD_UNCERTAIN_S{sell_votes:.1f}_B{buy_votes:.1f}"
                
                labels.append(label)
                label_justifications.append(justification)
            
            df['label'] = labels
            df['label_justification'] = label_justifications
            
            # Limpiar datos
            df = df.dropna(subset=[f'change_{h}' for h in horizons])
            
            # === AN√ÅLISIS INICIAL ===
            initial_counts = Counter(labels)
            total = len([l for l in labels if not pd.isna(l)])
            
            print("üìä Distribuci√≥n inicial de labels:")
            for i, name in enumerate(self.class_names):
                count = initial_counts[i]
                pct = count / total * 100 if total > 0 else 0
                print(f"   - {name}: {count} ({pct:.1f}%)")
            
            # === BALANCE S√öPER FORZADO ===
            min_count = min(initial_counts.values())
            max_count = max(initial_counts.values())
            
            if max_count / min_count > 1.5:  # Si hay desbalance > 50%
                print("\n‚öñÔ∏è Aplicando balance s√∫per forzado...")
                
                target_size = int(min_count * 1.2)  # Cada clase un 20% m√°s que la m√≠nima
                
                balanced_dfs = []
                
                for label_id in [0, 1, 2]:
                    label_data = df[df['label'] == label_id]
                    
                    if len(label_data) >= target_size:
                        # Selecci√≥n estratificada por r√©gimen
                        regime_samples = []
                        
                        for regime_id in [0, 1, 2]:
                            regime_label_data = label_data[label_data['regime'] == regime_id]
                            if len(regime_label_data) > 0:
                                n_samples = min(len(regime_label_data), target_size // 3)
                                regime_samples.append(
                                    regime_label_data.sample(n=n_samples, random_state=42)
                                )
                        
                        if regime_samples:
                            balanced_data = pd.concat(regime_samples, ignore_index=True)
                        else:
                            balanced_data = label_data.sample(
                                n=min(target_size, len(label_data)), 
                                random_state=42
                            )
                    else:
                        # Conservar todos los datos de clases minoritarias
                        balanced_data = label_data
                    
                    balanced_dfs.append(balanced_data)
                
                df = pd.concat(balanced_dfs, ignore_index=True)
                df = df.sample(frac=1, random_state=42).reset_index(drop=True)
                
                # === ESTAD√çSTICAS FINALES ===
                final_counts = Counter(df['label'])
                final_total = len(df)
                
                print("‚úÖ Balance s√∫per final:")
                for i, name in enumerate(self.class_names):
                    count = final_counts[i]
                    pct = count / final_total * 100
                    print(f"   - {name}: {count} ({pct:.1f}%)")
                
                # Calcular bias score
                max_pct = max([final_counts[i]/final_total for i in range(3)])
                min_pct = min([final_counts[i]/final_total for i in range(3)])
                bias_score = (max_pct - min_pct) * 10  # Score 0-10
                
                print(f"üìè Bias Score Final: {bias_score:.1f}/10 (0=perfecto)")
                
                if bias_score < 2.0:
                    print("üéØ ¬°BALANCE PERFECTO LOGRADO!")
                elif bias_score < 4.0:
                    print("‚úÖ Balance excelente logrado")
                else:
                    print("‚ö†Ô∏è Balance bueno, pero mejorable")
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return df
    
    async def run_super_training(self):
        """Ejecuta el entrenamiento s√∫per avanzado completo"""
        print("\n" + "="*80)
        print("üöÄ INICIANDO SUPER BALANCED TCN TRAINING")
        print("="*80)
        
        # === ETAPA 1: SETUP ===
        print("\nüìã ETAPA 1: CONFIGURACI√ìN")
        if not self.setup_binance_client():
            return False
        
        # === ETAPA 2: DATOS ===
        print("\nüìä ETAPA 2: RECOLECCI√ìN DE DATOS")
        timeframe_data = self.collect_professional_data()
        if not timeframe_data:
            return False
        
        # Usar el timeframe con m√°s datos
        main_timeframe = max(timeframe_data.keys(), key=lambda k: len(timeframe_data[k]))
        df = timeframe_data[main_timeframe]
        
        print(f"\n‚úÖ Usando timeframe principal: {main_timeframe} ({len(df)} per√≠odos)")
        
        # === ETAPA 3: REG√çMENES ===
        print("\nüîç ETAPA 3: DETECCI√ìN DE REG√çMENES")
        df = self.detect_professional_regimes(df)
        
        # === ETAPA 4: FEATURES ===
        print("\nüîß ETAPA 4: INGENIER√çA DE FEATURES")
        df = self.create_consistent_features(df)
        
        if df is None or len(df) < 200:
            print("‚ùå Features insuficientes")
            return False
        
        # === ETAPA 5: AUMENTACI√ìN ===
        print("\nüîß ETAPA 5: AUMENTACI√ìN DE DATOS")
        df = self.apply_professional_augmentation(df)
        
        # === ETAPA 6: LABELS ===
        print("\nüéØ ETAPA 6: CREACI√ìN DE LABELS")
        df = self.create_super_balanced_labels(df)
        
        # === ETAPA 7: PREPARACI√ìN ===
        print("\nüì¶ ETAPA 7: PREPARACI√ìN DE DATOS")
        X_features, X_regimes, y, feature_columns = self.prepare_training_data(df)
        
        if X_features is None:
            return False
        
        # === ETAPA 8: MODELO ===
        print("\nüß† ETAPA 8: CREACI√ìN DEL MODELO")
        model = self.create_super_tcn_model()
        
        if model is None:
            return False
        
        # === ETAPA 9: ENTRENAMIENTO REAL ===
        print("\nüî• ETAPA 9: ENTRENAMIENTO DEL MODELO")
        
        # Split train/validation
        split_idx = int(len(X_features) * 0.8)
        
        X_features_train = X_features[:split_idx]
        X_regimes_train = X_regimes[:split_idx]
        y_train = y[:split_idx]
        
        X_features_val = X_features[split_idx:]
        X_regimes_val = X_regimes[split_idx:]
        y_val = y[split_idx:]
        
        print(f"üìä Split de datos:")
        print(f"   - Train: {len(X_features_train)} muestras")
        print(f"   - Validation: {len(X_features_val)} muestras")
        
        # Callbacks
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=0.00001,
                verbose=1
            )
        ]
        
        # ENTRENAMIENTO REAL
        print("\nüöÄ Iniciando entrenamiento...")
        
        history = model.fit(
            [X_features_train, X_regimes_train],
            y_train,
            validation_data=([X_features_val, X_regimes_val], y_val),
            epochs=50,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        # === ETAPA 10: EVALUACI√ìN ===
        print("\nüìä ETAPA 10: EVALUACI√ìN DEL MODELO")
        
        # Predicciones
        val_predictions = model.predict([X_features_val, X_regimes_val])
        val_pred_classes = np.argmax(val_predictions, axis=1)
        
        # Distribuci√≥n de predicciones
        pred_counts = Counter(val_pred_classes)
        total_preds = len(val_pred_classes)
        
        print("üìä Distribuci√≥n de predicciones en validaci√≥n:")
        for i, name in enumerate(self.class_names):
            count = pred_counts[i]
            pct = count / total_preds * 100
            print(f"   - {name}: {count} ({pct:.1f}%)")
        
        # Calcular bias score final
        max_pct = max([pred_counts[i]/total_preds for i in range(3)])
        min_pct = min([pred_counts[i]/total_preds for i in range(3)])
        bias_score = (max_pct - min_pct) * 10
        
        print(f"\nüéØ BIAS SCORE FINAL: {bias_score:.1f}/10")
        
        if bias_score < 2.0:
            print("üèÜ ¬°BALANCE PERFECTO LOGRADO!")
        elif bias_score < 4.0:
            print("‚úÖ Balance excelente logrado")
        else:
            print("‚ö†Ô∏è Balance bueno, requiere mejora")
        
        # === ETAPA 11: GUARDAR MODELO ===
        print("\nüíæ ETAPA 11: GUARDANDO MODELO")
        
        # Guardar modelo
        model.save('models/super_balanced_tcn_final.h5')
        
        # Guardar metadata
        metadata = {
            'model_type': 'Super Balanced TCN',
            'features': len(feature_columns),
            'training_samples': len(X_features_train),
            'validation_samples': len(X_features_val),
            'bias_score': float(bias_score),
            'final_distribution': {
                'SELL': float(pred_counts[0]/total_preds),
                'HOLD': float(pred_counts[1]/total_preds),
                'BUY': float(pred_counts[2]/total_preds)
            },
            'techniques_applied': [
                'Multi-timeframe data',
                'Professional regime detection',
                'Noise injection augmentation',
                'Super balanced labels',
                'Optimized TCN architecture',
                'Sharpe Ratio loss function',
                'Forced absolute balance'
            ]
        }
        
        with open('models/super_balanced_metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"‚úÖ Modelo guardado en: models/super_balanced_tcn_final.h5")
        print(f"‚úÖ Metadata guardado en: models/super_balanced_metadata.json")
        
        print("\n" + "="*80)
        print("üéâ SUPER BALANCED TCN ENTRENAMIENTO COMPLETADO")
        print("="*80)
        print("‚úÖ T√©cnicas implementadas:")
        print("   üîπ Multi-timeframe data collection")
        print("   üîπ Detecci√≥n profesional de reg√≠menes")
        print("   üîπ Aumentaci√≥n con noise injection")
        print("   üîπ Labels s√∫per balanceadas")
        print("   üîπ Arquitectura TCN optimizada")
        print("   üîπ Loss function Sharpe Ratio")
        print("   üîπ Balance forzado absoluto")
        print("   üîπ ENTRENAMIENTO REAL COMPLETADO ‚úÖ")
        
        return True
    
    # === M√âTODOS AUXILIARES === 
    
    def create_consistent_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Crea el conjunto EXACTO y consistente de 66 features t√©cnicas"""
        try:
            print("\nüîß Creando features consistentes...")
            
            df = df.copy()
            
            # === LISTA FIJA DE 66 FEATURES PARA CONSISTENCIA ===
            self.FIXED_FEATURE_LIST = [
                'open', 'high', 'low', 'close', 'volume',  # 5 OHLCV
                'sma_5', 'sma_7', 'sma_10', 'sma_14', 'sma_20', 'sma_25', 'sma_30', 'sma_50', 'sma_100', 'sma_200',  # 10 SMA
                'ema_5', 'ema_9', 'ema_12', 'ema_21', 'ema_26', 'ema_50', 'ema_100', 'ema_200',  # 8 EMA
                'rsi_9', 'rsi_14', 'rsi_21', 'rsi_30',  # 4 RSI
                'macd', 'macd_signal', 'macd_histogram', 'macd_normalized', 'macd_signal_normalized', 'macd_histogram_normalized',  # 6 MACD
                'bb_upper_20', 'bb_lower_20', 'bb_position_20', 'bb_upper_50', 'bb_lower_50', 'bb_position_50',  # 6 BB
                'momentum_3', 'roc_3', 'momentum_5', 'roc_5', 'momentum_10', 'roc_10', 'momentum_20', 'roc_20',  # 8 Momentum
                'volatility_5', 'volatility_10', 'volatility_20', 'volatility_50',  # 4 Volatility
                'volume_sma_5', 'volume_ratio_5', 'volume_sma_10', 'volume_ratio_10', 'volume_sma_20', 'volume_ratio_20',  # 6 Volume
                'atr_14', 'atr_21', 'atr_30',  # 3 ATR
                'stoch_k', 'stoch_d',  # 2 Stochastic
                'williams_r_14', 'williams_r_21',  # 2 Williams %R
                'price_position_10', 'price_distance_ma_10', 'price_position_20', 'price_distance_ma_20',  # 4 Price position
                'close_change', 'volume_change'  # 2 Change features
            ]
            
            # Forzar exactamente 66 features
            self.FIXED_FEATURE_LIST = self.FIXED_FEATURE_LIST[:66]
            
            print(f"‚úÖ Lista fija definida: {len(self.FIXED_FEATURE_LIST)} features")
            
            # Ahora SOLO CALCULAR las features, NO agregar a la lista
            # La lista ya est√° definida y fija
            
            # 1. OHLCV b√°sicos - ya est√°n en el DataFrame
            
            # 2. Moving Averages SMA (10 features)
            for period in [5, 7, 10, 14, 20, 25, 30, 50, 100, 200]:
                df[f'sma_{period}'] = df['close'].rolling(window=period).mean()
            
            # 3. Exponential Moving Averages (8 features)
            for period in [5, 9, 12, 21, 26, 50, 100, 200]:
                df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
            
            # 4. RSI m√∫ltiples per√≠odos (4 features)
            for period in [9, 14, 21, 30]:
                delta = df['close'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
                rs = gain / loss
                df[f'rsi_{period}'] = 100 - (100 / (1 + rs))
            
            # 5. MACD completo (6 features)
            df['macd'] = df['ema_12'] - df['ema_26']
            df['macd_signal'] = df['macd'].ewm(span=9).mean()
            df['macd_histogram'] = df['macd'] - df['macd_signal']
            df['macd_normalized'] = df['macd'] / df['close']
            df['macd_signal_normalized'] = df['macd_signal'] / df['close']
            df['macd_histogram_normalized'] = df['macd_histogram'] / df['close']
            
            # 6. Bollinger Bands (6 features)
            for period in [20, 50]:
                bb_middle = df['close'].rolling(window=period).mean()
                bb_std = df['close'].rolling(window=period).std()
                df[f'bb_upper_{period}'] = bb_middle + (bb_std * 2)
                df[f'bb_lower_{period}'] = bb_middle - (bb_std * 2)
                df[f'bb_position_{period}'] = (df['close'] - df[f'bb_lower_{period}']) / (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}'])
            
            # 7. Momentum y ROC (8 features)
            for period in [3, 5, 10, 20]:
                df[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
                df[f'roc_{period}'] = df['close'].pct_change(periods=period)
            
            # 8. Volatilidad (4 features)
            for period in [5, 10, 20, 50]:
                df[f'volatility_{period}'] = df['close'].pct_change().rolling(window=period).std()
            
            # 9. Volume analysis (6 features)
            for period in [5, 10, 20]:
                df[f'volume_sma_{period}'] = df['volume'].rolling(window=period).mean()
                df[f'volume_ratio_{period}'] = df['volume'] / df[f'volume_sma_{period}']
            
            # 10. ATR (Average True Range) (3 features)
            for period in [14, 21, 30]:
                high_low = df['high'] - df['low']
                high_close = (df['high'] - df['close'].shift()).abs()
                low_close = (df['low'] - df['close'].shift()).abs()
                true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
                df[f'atr_{period}'] = true_range.rolling(window=period).mean()
            
            # 11. Stochastic Oscillator (2 features)
            low_min = df['low'].rolling(window=14).min()
            high_max = df['high'].rolling(window=14).max()
            df['stoch_k'] = 100 * (df['close'] - low_min) / (high_max - low_min)
            df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()
            
            # 12. Williams %R (2 features)
            for period in [14, 21]:
                high_max = df['high'].rolling(window=period).max()
                low_min = df['low'].rolling(window=period).min()
                df[f'williams_r_{period}'] = -100 * (high_max - df['close']) / (high_max - low_min)
            
            # 13. Price position features (4 features)
            for period in [10, 20]:
                df[f'price_position_{period}'] = (df['close'] - df['low'].rolling(period).min()) / \
                                                (df['high'].rolling(period).max() - df['low'].rolling(period).min())
                df[f'price_distance_ma_{period}'] = (df['close'] - df['close'].rolling(period).mean()) / df['close']
            
            # 14. Features adicionales para completar 66
            df['close_change'] = df['close'].pct_change()
            df['volume_change'] = df['volume'].pct_change()
            
            # USAR SOLO LA LISTA FIJA DE 66 FEATURES
            df = df.dropna()
            
            print(f"‚úÖ Features fijas creadas: {len(self.FIXED_FEATURE_LIST)}")
            print(f"   - Datos limpios: {len(df)} per√≠odos")
            
            # Verificar que todas las features existen
            missing_features = []
            for feature in self.FIXED_FEATURE_LIST:
                if feature not in df.columns:
                    missing_features.append(feature)
                    df[feature] = 0.0  # Feature dummy si falta
            
            if missing_features:
                print(f"‚ö†Ô∏è Features faltantes completadas con 0: {len(missing_features)}")
            
            # Retornar solo las columnas necesarias en orden fijo
            result_columns = ['timestamp', 'regime'] + self.FIXED_FEATURE_LIST
            return df[result_columns]
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None
    
    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Prepara datos para entrenamiento con arquitectura dual"""
        try:
            print("\nüì¶ Preparando datos para entrenamiento...")
            
            # Usar la lista fija de features para garantizar consistencia
            feature_columns = self.FIXED_FEATURE_LIST
            
            print(f"‚úÖ Usando features fijas: {len(feature_columns)} features")
            
            # Normalizar features
            scaler = MinMaxScaler()
            df_scaled = df.copy()
            df_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])
            
            # Crear directorio para modelos
            import os
            os.makedirs('models', exist_ok=True)
            
            # Guardar scaler
            with open('models/super_feature_scalers.pkl', 'wb') as f:
                pickle.dump(scaler, f)
            
            # Crear secuencias temporales
            X_features = []
            X_regimes = []
            y = []
            
            for i in range(self.lookback_window, len(df_scaled)):
                # Features temporales
                feature_seq = df_scaled[feature_columns].iloc[i-self.lookback_window:i].values
                X_features.append(feature_seq)
                
                # R√©gimen actual
                regime = df_scaled['regime'].iloc[i]
                regime_one_hot = np.zeros(3)
                regime_one_hot[int(regime)] = 1
                X_regimes.append(regime_one_hot)
                
                # Label
                y.append(int(df_scaled['label'].iloc[i]))
            
            X_features = np.array(X_features)
            X_regimes = np.array(X_regimes)
            y = np.array(y)
            
            print(f"‚úÖ Datos preparados:")
            print(f"   - X_features shape: {X_features.shape}")
            print(f"   - X_regimes shape: {X_regimes.shape}")
            print(f"   - y shape: {y.shape}")
            
            # Verificar distribuci√≥n final
            final_counts = Counter(y)
            total = len(y)
            print("üìä Distribuci√≥n final para entrenamiento:")
            for i, name in enumerate(self.class_names):
                count = final_counts[i]
                pct = count / total * 100
                print(f"   - {name}: {count} ({pct:.1f}%)")
            
            return X_features, X_regimes, y, feature_columns
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None, None, None, None
    
    def create_super_tcn_model(self) -> tf.keras.Model:
        """Crear modelo TCN s√∫per optimizado"""
        try:
            print("\nüß† Creando modelo TCN s√∫per optimizado...")
            
            # Inputs duales
            features_input = tf.keras.Input(shape=(self.lookback_window, self.expected_features), name='features')
            regimes_input = tf.keras.Input(shape=(3,), name='regimes')
            
            # === TCN SIMPLIFICADO Y ESTABLE ===
            x = features_input
            
            # Capas TCN m√°s simples pero efectivas
            x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='causal', activation='relu')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Dropout(0.2)(x)
            
            x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, dilation_rate=2, padding='causal', activation='relu')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Dropout(0.3)(x)
            
            x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, dilation_rate=4, padding='causal', activation='relu')(x)
            x = tf.keras.layers.BatchNormalization()(x)
            x = tf.keras.layers.Dropout(0.3)(x)
            
            # Global pooling simple
            x = tf.keras.layers.GlobalAveragePooling1D()(x)
            
            # === PROCESAMIENTO DE REG√çMENES SIMPLIFICADO ===
            regime_processed = tf.keras.layers.Dense(32, activation='relu')(regimes_input)
            regime_processed = tf.keras.layers.Dropout(0.2)(regime_processed)
            
            # === FUSI√ìN SIMPLE ===
            combined = tf.keras.layers.concatenate([x, regime_processed])
            
            # Capas densas finales simplificadas
            combined = tf.keras.layers.Dense(128, activation='relu')(combined)
            combined = tf.keras.layers.BatchNormalization()(combined)
            combined = tf.keras.layers.Dropout(0.4)(combined)
            
            combined = tf.keras.layers.Dense(64, activation='relu')(combined)
            combined = tf.keras.layers.Dropout(0.3)(combined)
            
            # Output con softmax
            outputs = tf.keras.layers.Dense(3, activation='softmax', name='predictions')(combined)
            
            # === COMPILACI√ìN CON LOSS PROFESIONAL ===
            model = tf.keras.Model(inputs=[features_input, regimes_input], outputs=outputs)
            
            model.compile(
                optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),
                loss='sparse_categorical_crossentropy',  # Loss 100% probado y estable
                metrics=['accuracy']
            )
            
            print(f"‚úÖ Modelo s√∫per TCN creado: {model.count_params():,} par√°metros")
            
            return model
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None


async def main():
    """Funci√≥n principal"""
    trainer = SuperBalancedTCN()
    success = await trainer.run_super_training()
    
    if success:
        print("\nüèÜ ¬°ENTRENAMIENTO S√öPER EXITOSO!")
        print("üéØ El modelo ahora deber√≠a estar PERFECTAMENTE BALANCEADO")
    else:
        print("\n‚ùå Error en el entrenamiento s√∫per")


if __name__ == "__main__":
    asyncio.run(main()) 